net :

  /* network name */
  string name_;

  /* The phase: TRAIN or TEST */
  Phase phase_;

  /* Individual layers in the net  */
  vector<shared_ptr<Layer<Dtype> > > layers_;

  /* layer names */
  vector<string> layer_names_;

  map<string, int> layer_names_index_;

 /* Vector of need backward of each net layer,
   * indexed by layer_id. */
  vector<bool> layer_need_backward_;

  /* the blobs storing intermediate results between the layer. */
  vector<shared_ptr<Blob<Dtype> > > blobs_;

  /* blob names */
  vector<string> blob_names_;
  map<string, int> blob_names_index_;

  /* vector is blobs , blob_id */
  vector<bool> blob_need_backward_;

  /* bottom_vecs stores the vectors containing the input for each layer.
   * They don't actually host the blobs (blobs_ does), so we simply store
   * pointers. */
  vector<vector<Blob<Dtype>*> > bottom_vecs_;

  /* With the bottom_vecs_ corresponding */
  vector<vector<int> > bottom_id_vecs_;

  /* With the bottom_vecs_ corresponding */
  vector<vector<bool> > bottom_need_backward_;

  /* top_vecs stores the vectors containing the output for each layer */
  vector<vector<Blob<Dtype>*> > top_vecs_;

  /* with the top_vecs_ corresponding */
  vector<vector<int> > top_id_vecs_;

  /* Vector of weight in the loss (or objective) function of each net blob,
   * indexed by blob_id. */
  vector<Dtype> blob_loss_weights_;

  vector<vector<int> > param_id_vecs_;

  vector<int> param_owners_;

  vector<string> param_display_names_;
  vector<pair<int, int> > param_layer_indices_;

  map<string, int> param_names_index_;

  /* blob indices for the input of the net */
  vector<int> net_input_blob_indices_;

  /* blob indices for the output of the net */
  vector<int> net_output_blob_indices_;

  vector<Blob<Dtype>*> net_input_blobs_;

  vector<Blob<Dtype>*> net_output_blobs_;

  /* The parameters in the network. */
  vector<shared_ptr<Blob<Dtype> > > params_;

  vector<Blob<Dtype>*> learnable_params_;

  /**
   * The mapping from params_ -> learnable_params_: we have
   * learnable_param_ids_.size() == params_.size(),
   * and learnable_params_[learnable_param_ids_[i]] == params_[i].get()
   * if and only if params_[i] is an "owner"; otherwise, params_[i] is a sharer
   * and learnable_params_[learnable_param_ids_[i]] gives its owner.
   */
  vector<int> learnable_param_ids_;

  /* the learning rate multipliers for learnable_params_ */
  vector<float> params_lr_;

  vector<bool> has_params_lr_;

  /* the weight decay multipliers for learnable_params_ */
  vector<float> params_weight_decay_;

  vector<bool> has_params_decay_;

  /* The bytes of memory used by this net */
  size_t memory_used_;

  /* Whether to compute and display debug info for the net. */
  bool debug_info_;

  // Callbacks
  vector<Callback*> before_forward_;
  vector<Callback*> after_forward_;
  vector<Callback*> before_backward_;
  vector<Callback*> after_backward_;







layer :

  /**
   * @brief Given the bottom blobs, compute the top blobs and the loss.
   *
   * @param bottom
   *     the input blobs, whose data fields store the input data for this layer
   * @param top
   *     the preshaped output blobs, whose data fields will store this layers'
   *     outputs
   * \return The total loss from the layer.

   * If the layer has any non-zero loss_weights, the wrapper
   * then computes and returns the loss.
   * Note :  Your layer should implement Forward_cpu and (optionally) Forward_gpu.
  Forward() 


  /**
   * @brief Given the top blob error gradients, compute the bottom blob error
   *        gradients.
   *
   * @param top
   *     the output blobs, whose diff fields store the gradient of the error
   *     with respect to themselves
   * @param propagate_down
   *     a vector with equal length to bottom, with each index indicating
   *     whether to propagate the error gradients down to the bottom blob at
   *     the corresponding index
   * @param bottom
   *     the input blobs, whose diff fields will store the gradient of the error
   *     with respect to themselves after Backward is run
   *
   * The Backward wrapper calls the relevant device wrapper function
   * (Backward_cpu or Backward_gpu) to compute the bottom blob diffs given the
   * top blob diffs.
   *
   * Your layer should implement Backward_cpu and (optionally) Backward_gpu.
   Backward()


  /** The protobuf that stores the layer parameters */
  LayerParameter layer_param_;

  /** The phase: TRAIN or TEST */
  Phase phase_;

  /** The vector that stores the learnable parameters as a set of blobs. */
  vector<shared_ptr<Blob<Dtype> > > blobs_;
  /** Vector indicating whether to compute the diff of each param blob. */
  vector<bool> param_propagate_down_;

  /** The vector that indicates whether each top blob has a non-zero weight in
   *  the objective function. */
  vector<Dtype> loss_;



blob :
	
```
对待处理数据带一层封装，用于在Caffe中通信传递。
也为CPU和GPU间提供同步能力
数学上，是一个N维的C风格的存储数组
总的来说，Caffe使用Blob来交流数据，其是Caffe中标准的数组与统一的内存接口，
```
  /* data_为原始数据,实际值要么存储在CPU(cpu_ptr_)要么存储在GPU(gpu_ptr_)*/
  shared_ptr<SyncedMemory> data_;

  /* diff_为梯度信息,实际值要么存储在CPU(cpu_ptr_)要么存储在GPU(gpu_ptr_)*/
  shared_ptr<SyncedMemory> diff_;
 
  /* shape_data_ 指向 cpu(cpu_ptr) 或gpu(gpu_prt)中 shape_ 的地址
  shared_ptr<SyncedMemory> shape_data_;

  /* shape_为blob维度 */
  vector<int> shape_;

  /* count为该blob中的元素总和（即数据的size），函数count(x,y)（或count(x)）返回某个切片[x,y]（[x,end]）内容量，本质上就是shape[x]shape[x+1]....*shape[y]的值 */
  int count_;

  /* 当前blob的个数 */
  int capacity_;


